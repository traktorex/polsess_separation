"""Tests for SPMamba model.

SPMamba requires CUDA as it uses mamba-ssm which depends on Triton.
All tests in this file will be skipped if CUDA is not available.

Uses reduced config parameters for memory efficiency (spmamba_sb_reduced.yaml).
"""

import pytest
import torch
from models import SPMamba


# SPMamba requires CUDA (mamba-ssm uses Triton)
pytestmark = pytest.mark.skipif(
    not torch.cuda.is_available(),
    reason="SPMamba requires CUDA (mamba-ssm dependency)",
)


@pytest.fixture
def device():
    """GPU device for SPMamba tests."""
    return "cuda"


@pytest.fixture
def spmamba_config():
    """Reduced SPMamba configuration for memory efficiency.
    
    Based on spmamba_sb_reduced.yaml to avoid memory overflow.
    """
    return {
        "n_fft": 256,  # Reduced from 512
        "stride": 64,  # Reduced from 128
        "input_dim": 64,
        "n_srcs": 1,
        "n_layers": 4,  # Reduced from 6
        "lstm_hidden_units": 192,  # Reduced from 256
        "attn_n_head": 2,  # Reduced from 4
        "attn_approx_qk_dim": 256,  # Reduced from 512
    }


def test_spmamba_initialization(spmamba_config):
    """Test SPMamba can be initialized with default params."""
    model = SPMamba(**spmamba_config)
    
    assert model is not None
    assert model.n_srcs == spmamba_config["n_srcs"]
    assert model.n_fft == spmamba_config["n_fft"]


def test_spmamba_forward_pass(spmamba_config, device):
    """Test SPMamba forward pass works correctly."""
    model = SPMamba(**spmamba_config).to(device)
    batch_size, time_steps = 1, 8000  # Reduced batch and length
    
    x = torch.randn(batch_size, time_steps).to(device)
    output = model(x)
    
    # Output should match input shape for enhancement
    assert output.shape == (batch_size, time_steps)
    assert not torch.isnan(output).any()
    assert not torch.isinf(output).any()


def test_spmamba_3d_input(spmamba_config, device):
    """Test SPMamba handles 3D input [batch, 1, time]."""
    model = SPMamba(**spmamba_config).to(device)
    batch_size, time_steps = 1, 8000
    
    x = torch.randn(batch_size, 1, time_steps).to(device)
    output = model(x)
    
    assert output.shape == (batch_size, time_steps)


def test_spmamba_separation_task(device):
    """Test SPMamba with n_srcs=2 (separation task)."""
    model = SPMamba(
        n_fft=256,
        stride=64,
        input_dim=64,
        n_srcs=2,  # Two sources
        n_layers=4,
        lstm_hidden_units=192,
        attn_n_head=2,
        attn_approx_qk_dim=256,
    ).to(device)
    
    batch_size, time_steps = 1, 8000  # Batch size 1
    x = torch.randn(batch_size, time_steps).to(device)
    output = model(x)
    
    # Should output 2 channels for separation
    assert output.shape == (batch_size, 2, time_steps)


def test_spmamba_different_layers(device):
    """Test SPMamba with different numbers of layers."""
    for n_layers in [2, 4]:  # Reduced range
        model = SPMamba(
            n_fft=256,
            stride=64,
            input_dim=64,
            n_srcs=1,
            n_layers=n_layers,
            lstm_hidden_units=128,  # Even more reduced
            attn_n_head=2,
            attn_approx_qk_dim=128,
        ).to(device)
        
        x = torch.randn(1, 8000).to(device)  # Batch size 1
        output = model(x)
        assert output.shape == (1, 8000)


def test_spmamba_different_nfft(device):
    """Test SPMamba with different STFT sizes."""
    for n_fft in [128, 256]:  # Reduced range
        model = SPMamba(
            n_fft=n_fft,
            stride=n_fft // 4,
            input_dim=64,
            n_srcs=1,
            n_layers=2,  # Minimal layers
            lstm_hidden_units=128,
            attn_n_head=2,
            attn_approx_qk_dim=128,
        ).to(device)
        
        x = torch.randn(1, 8000).to(device)  # Batch size 1
        output = model(x)
        assert output.shape == (1, 8000)


def test_spmamba_parameter_count(spmamba_config):
    """Test SPMamba has reasonable number of parameters."""
    model = SPMamba(**spmamba_config)
    num_params = sum(p.numel() for p in model.parameters())
    
    # SPMamba should have parameters in reasonable range
    assert num_params > 10_000  # At least some params
    assert num_params < 50_000_000  # Not too large


def test_spmamba_short_input(device):
    """Test SPMamba handles shorter inputs."""
    model = SPMamba(
        n_fft=256,
        stride=64,
        input_dim=64,
        n_srcs=1,
        n_layers=2,
        lstm_hidden_units=128,
        attn_n_head=2,
        attn_approx_qk_dim=128,
    ).to(device)
    
    # Shorter input
    x = torch.randn(1, 4000).to(device)  # Batch size 1
    output = model(x)
    
    # Output should match input length
    assert output.shape == (1, 4000)


def test_spmamba_preserves_input_length(device):
    """Test that SPMamba preserves input length (STFT reconstruction)."""
    model = SPMamba(
        n_fft=256,
        stride=64,
        input_dim=64,
        n_srcs=1,
        n_layers=2,
        lstm_hidden_units=128,
        attn_n_head=2,
        attn_approx_qk_dim=128,
    ).to(device)
    
    # Test various input lengths
    for length in [8000, 16000, 24000]:
        x = torch.randn(1, length).to(device)
        output = model(x)
        
        # STFT-based models should preserve length exactly or within one frame
        assert output.shape[1] == length, f"Length mismatch: {output.shape[1]} != {length}"


def test_spmamba_bidirectional_processing(device):
    """Test SPMamba bidirectional selective scan (per paper).
    
    Paper states: Uses bidirectional Mamba modules to process both
    forward and backward sequences for non-causal separation,
    utilizing both past and future contextual information.
    """
    model = SPMamba(
        n_fft=256, stride=64, input_dim=64, n_srcs=1,
        n_layers=2, lstm_hidden_units=128,
        attn_n_head=2, attn_approx_qk_dim=128
    ).to(device)
    
    # Test that model uses context from both directions
    x = torch.randn(1, 16000).to(device)
    output = model(x)
    
    # Bidirectional processing should work
    assert output.shape == (1, 16000)
    assert torch.abs(output).mean() > 0.001  # Model is processing


def test_spmamba_linear_complexity(device):
    """Test SPMamba handles very long sequences efficiently (per paper).
    
    Paper states: Linear computational complexity O(L) enables efficient
    processing of long audio sequences, unlike Transformers' O(LÂ²).
    Demonstrates 2.42 dB improvement on Echo2Mix with reduced complexity.
    """
    model = SPMamba(
        n_fft=256, stride=64, input_dim=64, n_srcs=1,
        n_layers=2, lstm_hidden_units=128,
        attn_n_head=2, attn_approx_qk_dim=128
    ).to(device)
    
    # Test with long sequence (5 seconds at 16kHz = 80k samples)
    # Reduced from 320k to avoid GPU memory overflow while still demonstrating
    # linear complexity advantage over transformers
    long_seq = torch.randn(1, 32000).to(device)
    output = model(long_seq)
    
    # Should handle efficiently with linear complexity
    assert output.shape == long_seq.shape
    assert not torch.isnan(output).any()
    assert not torch.isinf(output).any()


def test_spmamba_selective_scan_mechanism(device):
    """Test SPMamba selective scan mechanism (per paper).
    
    Paper states: Selective SSM with input-dependent dynamics allows
    model to selectively propagate or forget information, focusing on
    relevant parts of the audio signal.
    """
    model = SPMamba(
        n_fft=256, stride=64, input_dim=64, n_srcs=1,
        n_layers=2, lstm_hidden_units=128,
        attn_n_head=2, attn_approx_qk_dim=128
    ).to(device)
    model.eval()
    
    # Silent vs noisy inputs should be processed differently
    # (selective mechanism adapts to input)
    silent = torch.zeros(1, 8000).to(device)
    noisy = torch.randn(1, 8000).to(device)
    
    with torch.no_grad():
        out_silent = model(silent)
        out_noisy = model(noisy)
    
    # Selective processing should treat inputs differently
    # (input-dependent dynamics)
    assert not torch.allclose(out_silent, out_noisy, atol=1e-5)
    
    # Both should produce valid outputs
    assert not torch.isnan(out_silent).any()
    assert not torch.isnan(out_noisy).any()

